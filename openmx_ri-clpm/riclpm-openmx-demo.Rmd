---
layout: post
comments: true
title: "The RI-CLPM in OpenMX"
author: "John Flournoy"
date: "September 26, 2018"
output: 
  md_document:
    toc: true
---

Lavaan is great. I love lavaan. But a lot of folks prefer [OpenMx](https://openmx.ssri.psu.edu/), and given its long and widespread usage, especially in fields outside psychology, it might be a bit better road tested. So this is a brief addendum to my previous post, showing you how to implement a RI-CLPM in OpenMx.

<!--more-->

# RI-CLPM

As a reminder, the model looks like this:

![RI-CLPM Diagram](/../figs/riclpm-lavaan-demo/hamaker-diagram.png)

# Implemmenting the RI-CLPM in R

```{r packages, message = F, warning = F}
#if you need to install anything, uncomment the below install lines for now
#install.packages('lavaan')
#install.packages('tidyverse')
require(OpenMx)
require(tidyverse)
```

## Some data

I'll use the same data from before, which was presented on at a methods symposium at SRCD in 1997. Supporting documentation can be found [in this pdf]({{ "/assets/pdf/srcdmeth.pdf" | absolute_url }}). Data and code for importing it was helpfully provided by [Sanjay Srivastava](http://twitter.com/hardsci).

The variables we're considering are a measure of antisocial behavior (`anti`) and reading recognition (`read`). See the docs for descriptions of the other variables. And for the purpose of the model fitting below, `x <- anit` and `y <- read`. Following are some descriptions of the raw data:

```{r, lavaan demo growth data}
antiread <- read.table("srcddata.dat",
                       na.strings = c("999.00"),
                       col.names = c("anti1", "anti2", "anti3", "anti4", 
                                     "read1", "read2", "read3", "read4",
                                     "gen", "momage", "kidage", "homecog", 
                                     "homeemo", "id")
) %>%
  rename(x1 = anti1, x2 = anti2, x3 = anti3, x4 = anti4,
         y1 = read1, y2 = read2, y3 = read3, y4 = read4) %>%
  select(matches('[xy][1-4]'))

knitr::kable(summary(antiread), format = 'markdown')
```

```{r Variable density, fig.width=6, fig.height=4}
antiread %>%
  select(-x4,-y4) %>%
  mutate(pid = 1:n()) %>%
  gather(key, value, -pid) %>%
  extract(col = key, into = c('var', 'wave'), regex = '(\\w)(\\d)') %>%
  ggplot(aes(x = value)) +
  geom_density(alpha = 1) + 
  facet_grid(wave~var, scales = 'free') + 
  theme_classic()
```

```{r Variables over time, fig.width=6, fig.height=4}
antireadLong <- antiread %>%
  select(-x4,-y4) %>%
  mutate(pid = 1:n()) %>%
  gather(key, value, -pid) %>%
  extract(col = key, into = c('var', 'wave'), regex = '(\\w)(\\d)')

antireadLong %>%
  ggplot(aes(x = wave, y = value, color = var, group = var)) +
  geom_point(position = position_jitter(w = .2), alpha = .1) +
  geom_line(stat = 'identity', aes(group = interaction(var, pid)), alpha = .04) + 
  geom_line(stat = 'smooth', method = 'lm', size = 1) + 
  theme_classic()

```

## Fitting a RI-CLPM in OpenMx


Below is the code to specify and fit the RI-CLPM with equality constraints (in lavaan [here](https://jflournoy.github.io/2017/10/20/riclpm-lavaan-demo/#adding-constraints-to-ri-clpm)) on the autoregressive, cross-lagged, and residual (for wave 2 and 3) paths. 

If you're not familiar with OpenMx syntax, but you know what a latent growth curve model looks like, check out the [OpenMx LGCM example](https://vipbg.vcu.edu/vipbg/OpenMx2/docs//OpenMx/latest/TimeSeries_Path.html). You construct your model using a series of functions to create each element. Note that OpenMx likes everything to be very explicit, which can be a good thing. You should think about and specify reasonable starting values, and you should be prepared to put bounds on variables that have them (e.g., variances shouldn't be negative). I'll call these out as they come up. Also, a lot of the functionality is specified through path labels, and so I'll introduce some shorthand to create labels with similar or identical names.

```{r}
antireadRaw <- mxData(observed = antiread, type = 'raw')
```

This specifies that the data is in the `antiread` data.frame, with 'raw' indicating that it is not a covariance matrix, but rather a data.frame with a row per observation, and a column per variable. It will look in the column names for any manifest variables that are specified.

```{r}
manifestsX <- c('x1', 'x2', 'x3')
manifestsY <- c('y1', 'y2', 'y3')

latentResX <- c('p1', 'p2', 'p3')
latentResY <- c('q1', 'q2', 'q3')
```

The above is just to be able to have single variables that contain all the manifest and latent variables I will be using for each construct. Note that these are _all_ the manifests, but not all the latents.

```{r}
kappa <- mxPath(from = "kappa", to = manifestsX, 
                arrows = 1, values = c(1,1,1), free = FALSE)
omega <- mxPath(from = "omega", to = manifestsY, 
                arrows = 1, values = c(1,1,1), free = FALSE)

print(omega)
```

This is how to specify the two latent random intercepts. Keep track of the names you use here, because you will need to indicate them in the model construction call. Also, note that printing the resulting variables gives helpful information you can check against your intended specification. The path weights are set to 1 and set to not be freed for estimation.

```{r}
latentInterceptVars <- mxPath(from = c('kappa', 'omega'), arrows = 2,
                              free = T, connect = 'unique.pairs',
                              labels = c('kappaVar',
                                         'koCovar',
                                         'omegaVar'),
                              values = c(1, 0, 1),
                              lbound = c(0, NA, 0))
```

This creates free variance and covariance paths between the random intercepts. The 'unique.pairs' option will generate a path between each unique combination of the names you use in the "from" parameter. Giving them labels will help you identify them in the output, but are otherwise not strictly necessary. Note that the starting values are set with variances at 1, and the covariance at 0 (because it could be positive or negative), and lower bounds are set on the variance parameters.  

```{r}
meanPathNames <- unlist(lapply(c('mu', 'pi'), paste, 1:3, sep = ''))
```

Instead of writing out each of the path labels for the means of each manifest variable, so I call the above. It's more characters than just writing it out, but maybe less prone to typos.

```{r}
intercepts <- mxPath(from = "one", to = c(manifestsX, manifestsY),
                     arrows = 1, free = TRUE,
                     labels = meanPathNames,
                     values = rep(c(1,3), each =3))
```

The name "one" is reserved to refer to the intercept, and we allow the paths to be estimated freely, labeled with the names created above. It's probably not necessary to set starting values here, but I glanced at the histograms above and gave it my best guess, just in case.

```{r}
latentRes <- mxPath(from = c(latentResX, latentResY), 
                    to = c(manifestsX, manifestsY),
                    arrows = 1, free = F, 
                    values = rep(1, 6),
                    connect = 'single')
```

This call creates the paths to the latent variables that are also sometimes called "latent residuals." Each of the latent variable paths is fixed to 1.

```{r}
arPaths <- mxPath(from = c(latentResX[1:2], latentResY[1:2]),
                  to = c(latentResX[2:3], latentResY[2:3]),
                  arrows = 1, free = T, connect = 'single',
                  labels = c(rep('alpha', 2), rep('delta', 2)),
                  values = .2)

lagPaths <- mxPath(from = c(latentResX[1:2], latentResY[1:2]),
                  to = c(latentResY[2:3], latentResX[2:3]),
                  arrows = 1, free = T, connect = 'single',
                  labels = c(rep('gamma', 2), rep('beta', 2)),
                  value = 0)

print(arPaths)
print(lagPaths)
```

These calls establish the paths between the latent residuals. In the first call the first two latent variables for X and Y are connected to the second two. In the second, call, the first two X and Y variables are connected to the second two Y and X variables, respectively, to establish the lagged relations. **Paths with the same labels are constrained to equality**, which is why these paths have the labels assigned as they do. The starting values are based on the expectation that the autoregressive relation is probably positive.

```{r}
resVar <- mxPath(from = c(latentResX, latentResY),
                 arrows = 2, free = T, 
                 labels = paste0(c(rep('u',3), rep ('v',3)),
                                 c('1', '', '')),
                 connect = 'single',
                 value = 1, lbound = 0)

resCovar <- mxPath(from = latentResX,
                   to = latentResY,
                   arrows = 2, free = T, 
                   labels = paste0(rep ('rescovar',3), 
                                   c('1', '', '')), 
                   connect = 'single',
                   value = 0)
```

Finally, the residuals have to be specified. Any path left unspecified will not be included in the model -- unlike `lavaan`, `OpenMx` doesn't assume anything about your model. Note that the code `paste0(c(rep('u',3), rep ('v',3)), c('1', '', ''))` results in the labels: `r paste0(c(rep('u',3), rep ('v',3)), c('1', '', ''))`. These labels constrain the residuals (or disturbances) from wave 2 and 3 to be equal, with those from wave 1 estimated freely. The bivariate covariance is similarly constrained via the labels generated by `paste0(rep ('rescovar',3), c('1', '', ''))`: `r paste0(rep ('rescovar',3), c('1', '', ''))`. Again, the 'value' and 'lbound' options are set to a reasonable values for the variance. The starting value for the covariances is also set explicitly, though `0` is the default.

```{r}
riclpm <- mxModel('RICLPM', type = 'RAM',
                  manifestVars = c(manifestsX, manifestsY),
                  latentVars = c(latentResX, latentResY, 
                                 'kappa', 'omega'),
                  antireadRaw,
                  kappa,
                  omega,
                  latentInterceptVars,
                  intercepts,
                  latentRes,
                  arPaths,
                  lagPaths,
                  resVar,
                  resCovar)
```

This call actually puts everything together, translating the path specification into estimable matrices. The first argument is the model name (whatever you wish to call it). After that, you must specify all manifest and latent variables that appear in the path specifications (except for "one"). The rest of the call includes all of the model constructors we generated in the above code (including the data specification).

```{r}
mxOption(NULL,"Default optimizer","CSOLNP")
riclpm_fit <- mxRun(riclpm)
```

This is how you run the model. The optimizer "CSOLNP" is already the default, but I've included this code because often convergence issues can be solved by changing the optimizer. Check out the help on the `mxOption` function for more info. Passing your model variable to the `mxRun` function is where the magic happens.

```{r}
summary(riclpm_fit)
```

If you compare the fitted model summary to the `lavaan` output in the previous post, you'll see they match up very nicely.

## Unconstraining and comparing fits 

Editing an OpenMx model is fairly straightforward. In order to compare the model with and without equality constraints, we can create new path definitions without the label constraints, then call the `mxModel` function again with the original model and the new path elements. It will overwrite the old paths with the new specifications.

```{r}
#free autoregressive and cross-lagged paths
arPaths_uc <- mxPath(
  from = c(latentResX[1:2], latentResY[1:2]),
  to = c(latentResX[2:3], latentResY[2:3]),
  arrows = 1, free = T, connect = 'single',
  labels = paste0(c(rep('alpha', 2), rep('delta', 2)), 1:2),
  values = .2)
lagPaths_uc <- mxPath(
  from = c(latentResX[1:2], latentResY[1:2]),
  to = c(latentResY[2:3], latentResX[2:3]),
  arrows = 1, free = T, connect = 'single',
  labels = paste0(c(rep('gamma', 2), rep('beta', 2)), 1:2),
  value = 0)

#free residuals
resVar_uc <- mxPath(
  from = c(latentResX, latentResY),
  arrows = 2, free = T, 
  labels = paste0(c(rep('u',3), rep ('v',3)),
                  1:3),
  connect = 'single',
  value = 1, lbound = 0)

resCovar_uc <- mxPath(
  from = latentResX,
  to = latentResY,
  arrows = 2, free = T, 
  labels = paste0(rep ('rescovar',3), 
                  1:3), 
  connect = 'single',
  value = 0)

riclpm_uc <- mxModel(riclpm, 
                     arPaths_uc, lagPaths_uc, 
                     resVar_uc, resCovar_uc,
                     name = "RICLPM UC")

summary(riclpm_uc_fit <- mxRun(riclpm_uc))
```

The comparison can be made using `mxCompare`:

```{r}
mxCompare(riclpm_uc_fit, riclpm_fit)
```

Note that the default output uses the degrees-of-freedom penalized AIC. The conclusion is still the same, though -- the constrained model fits more poorly, significantly so if you believe the statistical test. 

Here are the estimated parameters:

```{r}
summary(riclpm_uc_fit)
```

You can also request standardized parameters (here, for the constrained model):

```{r}
mxStandardizeRAMpaths(riclpm_fit, SE = T)
```

## Plotting model fit

There are just a couple of changes to the code from the previous post having to do with how we extract information from the fitted model.

```{r Plot predictions, fig.width=10, fig.height=6, message = F, warning = F}
#get the model-expected means
means <- mxGetExpected(riclpm_fit, component = 'means')
meansDF <- data.frame(mean = means[1,], key = dimnames(means)[[2]]) %>%
  extract(col = key, into = c('var', 'wave'), regex = '(\\w)(\\d)')

factorScores <- mxFactorScores(riclpm_fit, type = 'regression', minManifests = 0)

#plot the model-expected random intercepts
as.data.frame(factorScores[,,1]) %>%
  mutate(pid = 1:n()) %>%
  gather(key, latentvalue, -pid, -kappa, -omega) %>%
  extract(col = key, into = c('latentvar', 'wave'), regex = '(\\w)(\\d)') %>%
  mutate(var = c(p = 'x', q = 'y')[latentvar]) %>%
  left_join(meansDF) %>% #those means from above
  left_join(antireadLong, by = c('pid', 'wave', 'var')) %>% #the raw data
  mutate(expectedLine = ifelse(var == 'x', kappa, omega) + mean,
         wave = as.numeric(wave)) %>%
  rowwise() %>%
  ggplot(aes(x = wave, y = expectedLine, color = var, group = var)) +
  geom_point(aes(x = wave, y = value, group = interaction(var, pid)), alpha = .2, position = position_jitter(w = .2, h = 0)) +
  geom_line(aes(y = expectedLine, group = interaction(var, pid)), stat = 'identity', alpha = .2) + 
  geom_line(aes(y = mean), stat = 'identity', alpha = 1, size = 1, color = 'black') + 
  facet_wrap(~var, ncol = 2) + 
  theme_classic()
```

The correlations between the latent residuals from this model are just about as easy to look at as the `lavaan` version.

```{r Plot clpm, fig.width=10, fig.height=10, message=F,warning=F}
library(GGally)
as.data.frame(factorScores[,,1]) %>%
  select(-kappa, -omega) %>%
  ggpairs(lower = list(continuous = wrap(ggally_smooth, alpha = .5))) + 
  theme_classic()
```

